# JudgeLRM: Large Reasoning Models as a Judge <a href='https://arxiv.org/abs/2504.00050'><img src='https://img.shields.io/badge/arXiv-2504.00050-b31b1b.svg'></a> &nbsp;

<p align="center">
  ðŸ“ƒ <a href="https://arxiv.org/abs/2504.00050" target="_blank">[Paper]</a> â€¢ ðŸ’» <a href="https://github.com/NuoJohnChen/JudgeLRM" target="_blank">[Github]</a> â€¢ ðŸ¤— <a href="https://huggingface.co/nuojohnchen/JudgeLRM-7B" target="_blank">[Models]</a> â€¢ <img src="https://osspicgo.oss-cn-shanghai.aliyuncs.com/hf_space_icon.svg"> <a href="https://huggingface.co/spaces/nuojohnchen/JudgeLRMDemo" target="_blank">[Playground]</a>
</p>

## Overview
JudgeLRM is a family of judgment-oriented Large Language Models (LLMs) designed to enhance evaluative reasoning through reinforcement learning (RL) with judge-wise, outcome-driven rewards. It demonstrates that judgment is inherently a reasoning-intensive task and addresses the limitations of supervised fine-tuning (SFT) in pair-wise evaluation. Notably, JudgeLRM-3B surpasses GPT-4, and JudgeLRM-7B outperforms DeepSeek-R1.

[Explore](https://huggingface.co/spaces/nuojohnchen/JudgeLRMDemo) JudgeLRMâ€™s reasoning capabilities and detailed comparisons by testing it against other Hugging Face models with your own questions!

## Acknowledgements
- [LogicRL](https://github.com/Unakar/Logic-RL) ðŸ”—
- [JudgeLM](https://github.com/Jiayi-Pan/TinyZero) ðŸ”—
- [PandaLM](https://github.com/AlphaPav/mem-kk-logic) ðŸ”—

------

## Citation
```
@misc{nuo2025judgelrm,
      title={JudgeLRM: Large Reasoning Models as a Judge}, 
      author={Nuo Chen, Zhiyuan Hu, Qingyun Zou, Jiaying Wu, Qian Wang, Bryan Hooi, Bingsheng He},
      year={2025},
      eprint={2504.00050},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2504.00050}, 
}
```
