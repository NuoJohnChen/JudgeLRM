# JudgeLRM: Large Reasoning Models as a Judge

<a href='https://arxiv.org/abs/2504.00050'><img src='https://img.shields.io/badge/arXiv-2504.00050-b31b1b.svg'></a> &nbsp;

## Overview
JudgeLRM is a family of judgment-oriented Large Language Models (LLMs) designed to enhance evaluative reasoning through reinforcement learning (RL) with judge-wise, outcome-driven rewards. It demonstrates that judgment is inherently a reasoning-intensive task and addresses the limitations of supervised fine-tuning (SFT) in pair-wise evaluation. Notably, JudgeLRM-3B surpasses GPT-4, and JudgeLRM-7B outperforms DeepSeek-R1.

Explore JudgeLRMâ€™s reasoning capabilities and detailed comparisons by testing it against other Hugging Face models with your own questions!

Demo: https://huggingface.co/spaces/nuojohnchen/JudgeLRMDemo

---

## Citation
```
@misc{nuo2025judgelrm,
      title={JudgeLRM: Large Reasoning Models as a Judge}, 
      author={Nuo Chen, Zhiyuan Hu, Qingyun Zou, Jiaying Wu, Qian Wang, Bryan Hooi, Bingsheng He},
      year={2025},
      eprint={2502.14768},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2504.00050}, 
}
```
